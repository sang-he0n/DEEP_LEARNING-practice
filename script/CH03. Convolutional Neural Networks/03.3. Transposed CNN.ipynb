{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH03.3. **Transposed CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. **작업 환경 설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.0. **사전 변수 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 2025\n",
    "BATCH_SIZE = 32\n",
    "EPOCH_NUM = 100\n",
    "USE_CHECKPOINT_YN = 'N'\n",
    "MODEL_PTH = '../../model/mnistAutoEncoder.pt'\n",
    "LOGGER_PTH = '../../log/mnistTrainLogger.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.1. **라이브러리 호출 및 옵션 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Device : mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7e4819ec7942ea8b2c380af39b9cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(1) Import libraries\n",
    "import os\n",
    "import ipywidgets\n",
    "import random\n",
    "import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "\n",
    "#(2) Set up options\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_NUM)\n",
    "random.seed(a=SEED_NUM)\n",
    "np.random.seed(seed=SEED_NUM)\n",
    "torch.use_deterministic_algorithms(mode=True)\n",
    "torch.manual_seed(seed=SEED_NUM)\n",
    "torch.mps.manual_seed(seed=SEED_NUM)\n",
    "\n",
    "#(3) Set up device\n",
    "if torch.backends.mps.is_available() :\n",
    "    device = torch.device(device='mps')\n",
    "else :\n",
    "    device = torch.device(device='cpu')\n",
    "print(f'>> Device : {device}')\n",
    "\n",
    "#(4) Set up HTML tag\n",
    "display(ipywidgets.HTML(data=\n",
    "'''\n",
    "<style> \n",
    "    .white-play button {\n",
    "        background-color: white !important; \n",
    "        color: black !important;\n",
    "    } \n",
    "</style>\n",
    "'''\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.2. **사용자정의함수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(df:torchvision.datasets, index:int) -> plt.figure :\n",
    "    img = df[index][0]\n",
    "    target = df[index][1]\n",
    "    img = (img/2+0.5).numpy() # -1 ~ 1 normalization \n",
    "    channel_cnt = img.shape[0]\n",
    "    if channel_cnt == 3 :\n",
    "        img = np.transpose(a=img, axes=(1, 2, 0))\n",
    "        plt.imshow(X=img) \n",
    "    elif channel_cnt == 1 : \n",
    "        img = np.squeeze(a=img, axis=0)\n",
    "        plt.imshow(X=img, cmap='gray')\n",
    "    else : \n",
    "        pass \n",
    "    plt.xlabel(xlabel=f'Target : {target}({df.classes[target]})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.3. **클래스 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Define `Flatten` class\n",
    "class Flatten(torch.nn.Module) :\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor :\n",
    "        '''\n",
    "            (batch size, channel size , (image) height, image width) -> (batch size, channel size * image width * image height) \n",
    "                                                                     -> (batch size, channel size * (image size)**2) \n",
    "        '''\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(shape=(batch_size, -1))\n",
    "        return x \n",
    "\n",
    "#(2) Define `Unflatten` class\n",
    "class Unflatten(torch.nn.Module) :\n",
    "    def __init__(self, channel_num:int) :\n",
    "        super().__init__()\n",
    "        self.channel_num = channel_num\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor :\n",
    "        ''' \n",
    "            (batch size, channel size * (image) height * width) -> (batch size, channel size, height, width) \n",
    "        '''\n",
    "        shape = x.shape\n",
    "        img_size = int((shape[1]//self.channel_num)**0.5) \n",
    "        x = x.reshape(shape=(shape[0], self.channel_num, img_size, img_size))\n",
    "        return  x \n",
    "    \n",
    "#(3) Define `MyConvAutoEncoder` class\n",
    "class MyConvAutoEncoder(torch.nn.Module) :\n",
    "    def __init__(self, input_shape:list, channel_num:int, class_num:int, device:torch.device) :\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.channel_num = channel_num\n",
    "        self.device = device\n",
    "        flattened_size = self._compute_flatten_size()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=channel_num, kernel_size=3, stride=2), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Conv2d(in_channels=channel_num, out_channels=2*channel_num, kernel_size=3, stride=2),\n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Conv2d(in_channels=2*channel_num, out_channels=4*channel_num, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            Flatten(),\n",
    "            torch.nn.Linear(in_features=flattened_size, out_features=class_num), \n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            Unflatten(channel_num=4*channel_num),\n",
    "            torch.nn.ConvTranspose2d(in_channels=4*channel_num, out_channels=2*channel_num, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels=2*channel_num, out_channels=channel_num, kernel_size=3, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels=channel_num, out_channels=1, kernel_size=3, stride=2, output_padding=1)\n",
    "        )\n",
    "        self.to(device=device)\n",
    "    def _compute_flatten_size(self) -> int :\n",
    "        with torch.no_grad() :\n",
    "            dummy_data = torch.zeros(size=(1, 1, self.input_shape[1], self.input_shape[2])).to(device=self.device)\n",
    "            dummy_fn = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=1, out_channels=self.channel_num, kernel_size=3, stride=2), \n",
    "                torch.nn.ReLU(), \n",
    "                torch.nn.Conv2d(in_channels=self.channel_num, out_channels=2*self.channel_num, kernel_size=3, stride=2),\n",
    "                torch.nn.ReLU(), \n",
    "                torch.nn.Conv2d(in_channels=2*self.channel_num, out_channels=4*self.channel_num, kernel_size=3, stride=1),\n",
    "                torch.nn.ReLU()\n",
    "            ).to(device=device)\n",
    "            output = dummy_fn(dummy_data)\n",
    "        output = output.reshape(shape=(1, -1)).shape[1]\n",
    "        return output\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor :\n",
    "        x = x.to(device=device)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "#(3) Define `TrainLogger` class\n",
    "class TrainLogger : \n",
    "    def __init__(self) :\n",
    "        self.train_log = {\n",
    "            'epoch'  : [],\n",
    "            'inputs' : [],\n",
    "            'preds'  : []\n",
    "        }\n",
    "    def log(self, epoch:int, inputs:torch.Tensor, preds:torch.Tensor, path:str) :\n",
    "        self.train_log['epoch'].append(epoch)\n",
    "        self.train_log['inputs'].append(inputs)\n",
    "        self.train_log['preds'].append(preds)\n",
    "        torch.save(obj={'train_log':self.train_log}, f=path)\n",
    "    def move_device(self, device:str) :\n",
    "        for i in range(len(self.train_log['inputs'])) :\n",
    "            if (device == 'cpu') :\n",
    "                self.train_log['inputs'][i] = self.train_log['inputs'][i].detach().cpu().numpy()\n",
    "                self.train_log['preds'][i] = self.train_log['preds'][i].detach().cpu().numpy()\n",
    "            else :\n",
    "                self.train_log['inputs'][i] = self.train_log['inputs'][i].to(device=device)\n",
    "                self.train_log['preds'][i] = self.train_log['preds'][i].to(device=device)\n",
    "\n",
    "#(4) Define `Visualizer` class\n",
    "class Visualizer :    \n",
    "    def __init__(self, train_log:dict, fig_size:tuple=(8, 8)) :\n",
    "        self.train_log = train_log\n",
    "        self.fig_size = fig_size\n",
    "        self.epoch_min = min(self.train_log['epoch'])\n",
    "        self.epoch_max = max(self.train_log['epoch'])\n",
    "        self.epoch_num = len(train_log['epoch']) - 1\n",
    "        self.sample_num = train_log['inputs'][0].shape[0] - 1\n",
    "        self.widget_output = ipywidgets.Output(\n",
    "            layout=ipywidgets.Layout(\n",
    "                width='auto', \n",
    "                height='auto', \n",
    "                margin='0px', \n",
    "                padding='0px'\n",
    "            )\n",
    "        )\n",
    "        self.epoch_play = ipywidgets.Play(\n",
    "            min=self.epoch_min,\n",
    "            max=self.epoch_max,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            interval=250,\n",
    "            description='Epoch Play',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.epoch_play.add_class(className='white-play')\n",
    "        self.epoch_slider = ipywidgets.IntSlider(\n",
    "            min=self.epoch_min,\n",
    "            max=self.epoch_max,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            description='Epoch',\n",
    "            continuous_update=True,\n",
    "            orientation='horizontal',\n",
    "            readout=True,\n",
    "            readout_format='d'\n",
    "        )\n",
    "        self.sample_slider = ipywidgets.IntSlider(\n",
    "            min=0,\n",
    "            max=self.sample_num,\n",
    "            step=1,\n",
    "            value=0,\n",
    "            description='Sample',\n",
    "            continuous_update=True,\n",
    "            orientation='horizontal',\n",
    "            readout=True,\n",
    "            readout_format='d'\n",
    "        )\n",
    "        ipywidgets.jslink(attr1=(self.epoch_play, 'value'), attr2=(self.epoch_slider, 'value'))\n",
    "        self.epoch_slider.observe(handler=self.on_epoch_change, names='value')\n",
    "        self.sample_slider.observe(handler=self.on_sample_change, names='value')\n",
    "        with self.widget_output:\n",
    "            self.fig, (self.ax1, self.ax2) = plt.subplots(nrows=1, ncols=2, figsize=self.fig_size)\n",
    "            try:\n",
    "                self.fig.canvas.header_visible = False\n",
    "                self.fig.canvas.toolbar_visible = False\n",
    "            except:\n",
    "                pass\n",
    "            plt.show()\n",
    "        self.update_view()\n",
    "    def on_epoch_change(self, change:dict) :\n",
    "        self.update_view()\n",
    "    def on_sample_change(self, change:dict) :\n",
    "        self.update_view()\n",
    "    def update_view(self) :\n",
    "        with self.widget_output:\n",
    "            self.ax1.clear()\n",
    "            self.ax2.clear()\n",
    "            ep_value = self.epoch_slider.value\n",
    "            ep_idx = self.train_log['epoch'].index(ep_value)\n",
    "            sp_idx = self.sample_slider.value\n",
    "            input_img = self.train_log['inputs'][ep_idx][sp_idx].squeeze()\n",
    "            pred_img = self.train_log['preds'][ep_idx][sp_idx].squeeze()\n",
    "            self.ax1.imshow(X=input_img, cmap='gray')\n",
    "            self.ax1.set_title(label='Target', fontdict={'fontsize': 12})\n",
    "            self.ax1.set_aspect(aspect='auto')\n",
    "            self.ax1.axis('off')\n",
    "            self.ax2.imshow(X=pred_img, cmap='gray')\n",
    "            self.ax2.set_title(label='Prediction', fontdict={'fontsize': 12})\n",
    "            self.ax2.set_aspect(aspect='auto')\n",
    "            self.ax2.axis('off')\n",
    "            self.fig.canvas.draw_idle()\n",
    "    def plot_compare(self) -> ipywidgets.widgets :\n",
    "        controls_box = ipywidgets.VBox(\n",
    "            children=[\n",
    "                self.epoch_slider,\n",
    "                self.sample_slider,\n",
    "                self.epoch_play\n",
    "            ],\n",
    "            layout=ipywidgets.Layout(\n",
    "                align_items='center',\n",
    "                margin='0% 0% 15% -5%'\n",
    "            )\n",
    "        )\n",
    "        ui = ipywidgets.HBox(\n",
    "            children=[self.widget_output, controls_box],\n",
    "            layout=ipywidgets.Layout(\n",
    "                justify_content='flex-start',\n",
    "                align_items='center',\n",
    "                width='auto',\n",
    "                margin='0px',\n",
    "                padding='0px'\n",
    "            )\n",
    "        )\n",
    "        display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. **데이터셋 전처리 및 로드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.1. **이미지 전처리 파이프라인 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tf = torchvision.transforms.Compose(\n",
    "    transforms=[\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.2. **데이터셋 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(root='../../data', train=True, download=True, transform=img_tf)\n",
    "mnist_test = torchvision.datasets.MNIST(root='../../data', train=False, download=True, transform=img_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.3. **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(1) Print sample of train\n",
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(2) Display image\n",
    "show_img(df=mnist_train, index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28, 28]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(3) Check `input_size`\n",
    "input_shape = list(mnist_train[0][0].shape)\n",
    "\n",
    "#(4) Print `input_size`\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>6131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  count\n",
       "1      0   5923\n",
       "3      1   6742\n",
       "5      2   5958\n",
       "6      3   6131\n",
       "2      4   5842\n",
       "0      5   5421\n",
       "7      6   5918\n",
       "8      7   6265\n",
       "9      8   5851\n",
       "4      9   5949"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(5) Print frequency of target class\n",
    "target_freq = collections.Counter()\n",
    "for i in range(len(mnist_train)):\n",
    "    input, target = mnist_train[i]\n",
    "    if isinstance(target, torch.Tensor) :\n",
    "        target = target.item()\n",
    "    target_freq[target] += 1\n",
    "pd.DataFrame(data=list(target_freq.items()), columns=['class', 'count']).sort_values(by='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.4. **데이터로더 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. **모델 구축 및 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.1. **하이퍼 파라미터 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_num = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.1. **모델 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyConvAutoEncoder                        [32, 1, 28, 28]           --\n",
       "├─Sequential: 1-1                        [32, 10]                  --\n",
       "│    └─Conv2d: 2-1                       [32, 16, 13, 13]          160\n",
       "│    └─ReLU: 2-2                         [32, 16, 13, 13]          --\n",
       "│    └─Conv2d: 2-3                       [32, 32, 6, 6]            4,640\n",
       "│    └─ReLU: 2-4                         [32, 32, 6, 6]            --\n",
       "│    └─Conv2d: 2-5                       [32, 64, 4, 4]            18,496\n",
       "│    └─ReLU: 2-6                         [32, 64, 4, 4]            --\n",
       "│    └─Flatten: 2-7                      [32, 1024]                --\n",
       "│    └─Linear: 2-8                       [32, 10]                  10,250\n",
       "│    └─ReLU: 2-9                         [32, 10]                  --\n",
       "├─Sequential: 1-2                        [32, 1, 28, 28]           --\n",
       "│    └─Linear: 2-10                      [32, 1024]                11,264\n",
       "│    └─ReLU: 2-11                        [32, 1024]                --\n",
       "│    └─Unflatten: 2-12                   [32, 64, 4, 4]            --\n",
       "│    └─ConvTranspose2d: 2-13             [32, 32, 6, 6]            18,464\n",
       "│    └─ReLU: 2-14                        [32, 32, 6, 6]            --\n",
       "│    └─ConvTranspose2d: 2-15             [32, 16, 13, 13]          4,624\n",
       "│    └─ReLU: 2-16                        [32, 16, 13, 13]          --\n",
       "│    └─ConvTranspose2d: 2-17             [32, 1, 28, 28]           145\n",
       "==========================================================================================\n",
       "Total params: 68,043\n",
       "Trainable params: 68,043\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 66.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 2.70\n",
       "Params size (MB): 0.27\n",
       "Estimated Total Size (MB): 3.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(1) Define `model`\n",
    "model = MyConvAutoEncoder(input_shape=input_shape, channel_num=channel_num, class_num=10, device=device).to(dtype=torch.float32)\n",
    "\n",
    "#(2) Display `model`\n",
    "torchinfo.summary(\n",
    "    model=model, \n",
    "    input_size=[BATCH_SIZE]+list(mnist_train[0][0].shape),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "#(5) Define optimizer(optimization method)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-7)\n",
    "\n",
    "#(6) Define Scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "#(7) Define logger\n",
    "logger = TrainLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.2. **학습 전 변수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch=0, Train Loss=inf\n"
     ]
    }
   ],
   "source": [
    "epoch = 0 \n",
    "loss_hist = []\n",
    "best_loss = float('inf')\n",
    "if USE_CHECKPOINT_YN == 'Y' :\n",
    "    try :\n",
    "        checkpoint = torch.load(f=MODEL_PTH, map_location=device)\n",
    "        model.load_state_dict(state_dict=checkpoint['model'])\n",
    "        optimizer.load_state_dict(state_dict=checkpoint['optimizer'])\n",
    "        epoch = checkpoint['best_epoch']\n",
    "        loss_hist = checkpoint['loss_hist']\n",
    "        best_loss = loss_hist[-1]\n",
    "        logger.train_log = torch.load(f=LOGGER_PTH, map_location=device)['train_log']\n",
    "    except Exception :\n",
    "        pass\n",
    "print(f\">> Epoch={epoch}, Train Loss={best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.3. **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:20<33:35, 20.36s/it, last_epoch=1, last_loss=0.0426, best_epoch=1, best_loss=0.0426]"
     ]
    }
   ],
   "source": [
    "batch_len = len(mnist_train_loader)\n",
    "best_epoch = epoch\n",
    "progress_bar = tqdm.trange(epoch+1, EPOCH_NUM+1)\n",
    "for epoch in progress_bar : \n",
    "    last_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(iterable=mnist_train_loader) :\n",
    "        optimizer.zero_grad() \n",
    "        preds = model(x=inputs)\n",
    "        # ---- Not use `targets` ---- #\n",
    "        loss = criterion(input=preds, target=inputs.to(device=device))\n",
    "        # ---- _________________ ---- #\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        last_loss += loss.item()\n",
    "        # ---- Logging first batch data ---- #\n",
    "        if batch_idx == 0 : \n",
    "            inputs_b_0 = inputs\n",
    "            preds_b_0 =  preds\n",
    "        # ---- _______________________ ---- # \n",
    "    logger.log(epoch=epoch, inputs=inputs_b_0, preds=preds_b_0, path=LOGGER_PTH)\n",
    "    \n",
    "    last_loss_avg = last_loss / batch_len\n",
    "    loss_hist.append(last_loss_avg)\n",
    "    if last_loss_avg < best_loss :\n",
    "        best_epoch = epoch\n",
    "        best_loss = last_loss_avg\n",
    "        torch.save(\n",
    "            obj={\n",
    "                'model'      : model.state_dict(),\n",
    "                'optimizer'  : optimizer.state_dict(),\n",
    "                'best_epoch' : best_epoch,\n",
    "                'loss_hist'  : loss_hist\n",
    "            }, \n",
    "            f=MODEL_PTH\n",
    "        )\n",
    "    # scheduler.step()\n",
    "    progress_bar.set_postfix(ordered_dict={'last_epoch':epoch, 'last_loss':last_loss_avg, 'best_epoch':best_epoch, 'best_loss':best_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. **모델 평가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.1. **최적 성능 모델 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f=MODEL_PTH, map_location=device)\n",
    "model.load_state_dict(state_dict=checkpoint['model'])\n",
    "print(f'>> Best Epoch : {np.argmin(a=checkpoint[\"loss_hist\"])+1}, Best Loss : {np.min(a=checkpoint[\"loss_hist\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04.2. **과소 적합 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlabel(xlabel='epoch')\n",
    "plt.ylabel(ylabel='loss')\n",
    "plt.plot(loss_hist, label='Training Loss')\n",
    "plt.axvline(x=np.argmin(a=checkpoint[\"loss_hist\"]), color='grey', linestyle='--', linewidth=0.6, label=f'Best Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.3. **(에포크 별) 학습 과정 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(0) Set up interactive mode\n",
    "%matplotlib widget\n",
    "\n",
    "#(1) Move device\n",
    "logger.move_device(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Define `visualizer`\n",
    "viz = Visualizer(train_log=logger.train_log, fig_size=(8, 4))\n",
    "\n",
    "#(3) Set up interactive mode\n",
    "viz.plot_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.4. **일반화 성능 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
