{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH04.4. **Bidirectional Sequence Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [][][][][]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. **작업 환경 설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.0. **사전 변수 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 2025\n",
    "BATCH_SIZE = 32\n",
    "EPOCH_NUM = 100\n",
    "USE_PRETRAIN_YN = 'N'\n",
    "MODEL_PTH = '../../model/mnistBiSeq.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.1. **라이브러리 호출 및 옵션 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Device : mps\n"
     ]
    }
   ],
   "source": [
    "#(1) Import libraries\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "\n",
    "#(2) Set up options\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_NUM)\n",
    "random.seed(a=SEED_NUM)\n",
    "np.random.seed(seed=SEED_NUM)\n",
    "torch.use_deterministic_algorithms(mode=True)\n",
    "torch.manual_seed(seed=SEED_NUM)\n",
    "torch.mps.manual_seed(seed=SEED_NUM)\n",
    "\n",
    "#(3) Set up device\n",
    "if torch.backends.mps.is_available() :\n",
    "    device = torch.device(device='mps')\n",
    "else :\n",
    "    device = torch.device(device='cpu')\n",
    "print(f'>> Device : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.2. **사용자정의함수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Define `show_img()` function\n",
    "def show_img(df:torchvision.datasets, index:int) :\n",
    "    img = df[index][0]\n",
    "    target = df[index][1]\n",
    "    img = img / 2 + 0.5    \n",
    "    plt.imshow(X=img) \n",
    "    plt.xlabel(xlabel=f'Target : {target}({df.classes[target]})')\n",
    "    plt.show()\n",
    "\n",
    "#(2) Define `compute_metrics()` function\n",
    "def compute_metrics(model:torch.nn.Module, loader:torch.utils.data.DataLoader) :\n",
    "    _preds = []\n",
    "    _targets = []\n",
    "    model.eval()\n",
    "    with torch.no_grad() : \n",
    "        for inputs, targets in loader :\n",
    "            preds = model(x=inputs)\n",
    "            preds = torch.argmax(input=preds, dim=1)\n",
    "            _preds.extend(preds.cpu().numpy())\n",
    "            _targets.extend(targets.cpu().numpy())\n",
    "    model.train()\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true=_targets, y_pred=_preds)\n",
    "    precision = sklearn.metrics.precision_score(y_true=_targets, y_pred=_preds, average='weighted')\n",
    "    recall = sklearn.metrics.recall_score(y_true=_targets, y_pred=_preds, average='weighted')\n",
    "    f1 = sklearn.metrics.f1_score(y_true=_targets, y_pred=_preds, average='weighted')\n",
    "    output = pd.DataFrame(data={\n",
    "        'metricName' : ['accuracy', 'precision', 'recall', 'f1'], \n",
    "        'value'      : [accuracy, precision, recall, f1] \n",
    "    })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 00.3. **클래스 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Define `MyClassificationModel` class\n",
    "class MyClassificationModel(torch.nn.Module) :\n",
    "    def __init__(self, input_size:int, hidden_size:int, seq_len:int, layers_num:int, class_num:int, device:torch.device) :\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.layers_num = layers_num\n",
    "        self.device = device\n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "        self.layers['lstm'] = torch.nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=layers_num, \n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.layers['fc'] = torch.nn.Linear(in_features=hidden_size*seq_len*2, out_features=class_num)\n",
    "        self.to(device=self.device)\n",
    "    def forward(self, x:torch.Tensor) :\n",
    "        h0 = torch.zeros(size=(self.layers_num*2, x.size()[0], self.hidden_size), device=self.device)\n",
    "        c0 = torch.zeros(size=(self.layers_num*2, x.size()[0], self.hidden_size), device=self.device)\n",
    "        x = x.to(device=self.device)\n",
    "        x = self.layers['lstm'](x, (h0, c0))[0]\n",
    "        x = x.reshape(shape=(x.shape[0], self.seq_len*self.hidden_size*2))\n",
    "        x = self.layers['fc'](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. **데이터셋 전처리 및 로드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.1. **이미지 전처리 파이프라인 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tf = torchvision.transforms.Compose(\n",
    "    transforms=[\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Lambda(lambda x: x.squeeze(0))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.2. **데이터셋 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(root='../../data', train=True, download=True, transform=img_tf)\n",
    "mnist_test = torchvision.datasets.MNIST(root='../../data', train=False, download=True, transform=img_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.3. **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(1) Print sample of train\n",
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(2) Print image shape \n",
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>6131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  count\n",
       "1      0   5923\n",
       "3      1   6742\n",
       "5      2   5958\n",
       "6      3   6131\n",
       "2      4   5842\n",
       "0      5   5421\n",
       "7      6   5918\n",
       "8      7   6265\n",
       "9      8   5851\n",
       "4      9   5949"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(3) Print frequency of target class\n",
    "target_freq = collections.Counter()\n",
    "for i in range(len(mnist_train)):\n",
    "    input, target = mnist_train[i]\n",
    "    if isinstance(target, torch.Tensor) :\n",
    "        target = target.item()\n",
    "    target_freq[target] += 1\n",
    "pd.DataFrame(data=list(target_freq.items()), columns=['class', 'count']).sort_values(by='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(4) Display image\n",
    "show_img(df=mnist_train, index=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01.4. **데이터로더 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. **모델 구축 및 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.1. **모델 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Define hyper-parameter\n",
    "seq_len = mnist_train[0][0].shape[0]\n",
    "input_size = mnist_train[0][0].shape[1]\n",
    "layers_num = 2\n",
    "hidden_size = 12\n",
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassificationModel(\n",
       "  (layers): ModuleDict(\n",
       "    (lstm): LSTM(28, 12, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=672, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(2) Define `model`\n",
    "model = MyClassificationModel(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    seq_len=seq_len, \n",
    "    layers_num=layers_num,\n",
    "    class_num=class_num,\n",
    "    device=device\n",
    ").to(dtype=torch.float32)\n",
    "\n",
    "#(3) Display `model`\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyClassificationModel                    [32, 10]                  --\n",
       "├─ModuleDict: 1-1                        --                        --\n",
       "│    └─LSTM: 2-1                         [32, 28, 24]              7,680\n",
       "│    └─Linear: 2-2                       [32, 10]                  6,730\n",
       "==========================================================================================\n",
       "Total params: 14,410\n",
       "Trainable params: 14,410\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 7.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.10\n",
       "Forward/backward pass size (MB): 0.17\n",
       "Params size (MB): 0.06\n",
       "Estimated Total Size (MB): 0.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(4)\n",
    "dummy = torch.randn(size=[BATCH_SIZE]+list(mnist_train[0][0].squeeze(dim=0).shape)).to(device=device)\n",
    "torchinfo.summary(model=model, input_data=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(5) Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#(6) Define optimizer(optimization method)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-2, weight_decay=1e-7)\n",
    "\n",
    "#(7) Define Scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.2. **모델 체크포인트 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch=0, Train Loss=inf\n"
     ]
    }
   ],
   "source": [
    "if USE_PRETRAIN_YN == 'Y' :\n",
    "    checkpoint = torch.load(f=MODEL_PTH)\n",
    "    model.load_state_dict(state_dict=checkpoint['model'])\n",
    "    optimizer.load_state_dict(state_dict=checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss_hist = checkpoint['loss_hist']\n",
    "    best_loss = loss_hist[-1]\n",
    "else :\n",
    "    epoch = 0\n",
    "    loss_hist = []\n",
    "    best_loss = float('inf')\n",
    "print(f\">> Epoch={epoch}, Train Loss={best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.3. **모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [07:53<44:22, 31.33s/it, epoch=15, loss=0.031] "
     ]
    }
   ],
   "source": [
    "batch_len = len(mnist_train_loader)\n",
    "progress_bar = tqdm.trange(epoch, EPOCH_NUM)\n",
    "for epoch in progress_bar : \n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in mnist_train_loader :\n",
    "        optimizer.zero_grad() \n",
    "        preds = model(x=inputs)\n",
    "        loss = criterion(input=preds, target=targets.to(device=device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    running_loss_avg = running_loss / batch_len\n",
    "    loss_hist.append(running_loss_avg)\n",
    "    if running_loss_avg < best_loss :\n",
    "        best_loss = running_loss_avg\n",
    "        torch.save(\n",
    "            obj={\n",
    "                'epoch'     : epoch,\n",
    "                'loss_hist' : loss_hist,\n",
    "                'model'     : model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict()\n",
    "            }, \n",
    "            f=MODEL_PTH\n",
    "        )\n",
    "    # scheduler.step()\n",
    "    progress_bar.set_postfix(ordered_dict={'epoch':epoch+1, 'loss':running_loss_avg}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. **모델 평가**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.1. **최적 성능 모델 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f=MODEL_PTH)\n",
    "model.load_state_dict(state_dict=checkpoint['model'])\n",
    "print(f'>> Epoch : {checkpoint[\"epoch\"]}, Loss : {checkpoint[\"loss_hist\"][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.2. **과소 적합 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Plot traing loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(label='Training Loss')\n",
    "plt.xlabel(xlabel='epoch')\n",
    "plt.ylabel(ylabel='loss')\n",
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Check metrics\n",
    "compute_metrics(model=model, loader=mnist_train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03.3. **일반화 성능 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(model=model, loader=mnist_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
